# configs/dqn_config.yaml

# --- Experiment ---
total_timesteps: 50000    # Total number of steps to train for (Change to 1_000_000 for full training)
device: "cuda"            # "cuda" or "cpu"

# --- Agent/Network ---
gamma: 0.98               # Discount factor for future rewards
learning_rate: 0.0001     # Learning rate for the Adam optimizer

# --- Environment ---
env_id: "ALE/MsPacman-v5"
frame_stack: 4
frame_height: 84
frame_width: 84

# --- Reward Shaping ---
enable_reward_shaping: true  # Whether to use reward shaping
enable_time_penalty: true  # Whether to apply a time penalty
time_penalty_per_step: -1  # Penalty to apply for each step taken
enable_death_penalty: true  # Whether to apply a death penalty
death_penalty: -100.0         # Penalty to apply when the agent loses a life
enable_level_completion_bonus: true  # Whether to apply a bonus for completing the level
level_completion_bonus: 5000.0  # Bonus to apply when the agent completes the level

# --- Replay Buffer & Learning Strategy ---
replay_buffer_capacity: 10000  # Max number of experiences to store
batch_size: 32                  # Number of experiences to sample for each learning step
learning_starts: 1000          # Number of steps to take before learning starts (to populate the buffer)
train_frequency: 4              # How often to perform a learning step (e.g., every 4 steps)

# --- Target Network ---
target_update_frequency: 500   # How often to update the target network (in steps)

# --- Epsilon-Greedy Exploration ---
epsilon_start: 1.0              # Starting value of epsilon
epsilon_end: 0.01               # Minimum value of epsilon
epsilon_decay_duration: 25000  # Number of steps over which to linearly decay epsilon

# --- Saving and Logging ---
save_frequency: 10000         # How often to save the model (in steps)
save_path: "models/dqn_checkpoints/"  # Path to save the trained model
log_path: "logs/"  # Path to save training logs
log_frequency: 1000  # How often to log training metrics (in steps)

# --- Game Play Videos ---
video_folder_path: "videos/"  # Folder to save video captures
capture_video: true            # Whether to capture video of the agent's performance
video_capture_steps: [50000, 100000, 500000, 1000000, 2000000, 5000000, 10000000]  # Steps at which to capture video