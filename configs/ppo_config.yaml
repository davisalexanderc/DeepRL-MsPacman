# configs/ppo_config.yaml

# --- Experiment ---
total_timesteps: 5000    # Total number of steps to train for (Change to 1_000_000 for full training)
device: "cuda"            # "cuda" or "cpu"

# --- Agent/Network ---
gamma: 0.98               # Discount factor for future rewards
learning_rate: 0.0001     # Learning rate for the Adam optimizer

# --- Environment ---
env_id: "ALE/MsPacman-v5"
frame_stack: 4
frame_height: 84
frame_width: 84

# --- Reward Shaping ---
enable_reward_shaping: true  # Whether to use reward shaping
enable_time_penalty: true  # Whether to apply a time penalty
time_penalty_per_step: -1  # Penalty to apply for each step taken
enable_death_penalty: true  # Whether to apply a death penalty
death_penalty: -100.0         # Penalty to apply when the agent loses a life
enable_level_completion_bonus: true  # Whether to apply a bonus for completing the level
level_completion_bonus: 5000.0  # Bonus to apply when the agent completes the level

# --- PPO Specific ---
num_steps: 128              # Number of steps to run for each update
num_epochs: 4               # Number of epochs to update the policy
num_mini_batches: 4       # Number of mini-batches to split the data into
clip_epsilon: 0.1          # Clipping parameter for PPO
gae_lambda: 0.95           # Lambda parameter for Generalized Advantage Estimation
entropy_coef: 0.01       # Coefficient for entropy bonus
value_loss_coef: 0.5       # Coefficient for value loss

# --- Saving and Logging ---
save_frequency: 1e4         # How often to save the model (in steps)
save_path: "models/ppo_checkpoints/"  # Path to save the trained model
log_path: "logs/"  # Path to save training logs
log_frequency: 1e3  # How often to log training metrics (in steps)

# --- Game Play Videos ---
video_folder_path: "videos/"  # Folder to save video captures
capture_video: true            # Whether to capture video of the agent's performance
video_capture_steps: [5e5, 1e5, 5e5, 1e6, 2e6, 5e6, 1e7]  # Steps at which to capture video