# configs/dqn_config.yaml

# --- Experiment ---
total_timesteps: 100000    # Total number of steps to train for (Change to 1_000_000 for full training)
device: "cuda"            # "cuda" or "cpu"

# --- Agent/Network ---
gamma: 0.99               # Discount factor for future rewards
learning_rate: 0.0001     # Learning rate for the Adam optimizer

# --- Replay Buffer & Learning Strategy ---
replay_buffer_capacity: 100000  # Max number of experiences to store
batch_size: 32                  # Number of experiences to sample for each learning step
learning_starts: 10000          # Number of steps to take before learning starts (to populate the buffer)
train_frequency: 4              # How often to perform a learning step (e.g., every 4 steps)

# --- Target Network ---
target_update_frequency: 1000   # How often to update the target network (in steps)

# --- Epsilon-Greedy Exploration ---
epsilon_start: 1.0              # Starting value of epsilon
epsilon_end: 0.01               # Minimum value of epsilon
epsilon_decay_duration: 250000  # Number of steps over which to linearly decay epsilon

# --- Saving ---
save_frequency: 50000           # How often to save the model (in steps)
save_path: "models/dqn_model.pth"  # Path to save the trained model